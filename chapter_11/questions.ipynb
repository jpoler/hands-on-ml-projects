{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npY-8SXjRChq"
   },
   "source": [
    "1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "\n",
    "No, because the network would have issues with symmetries between neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wV5tWXLHSWjo"
   },
   "source": [
    "2. Is it OK to initialize the bias terms to 0?\n",
    "\n",
    "Yes, bias terms can be set to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwQyN_PeSb-P"
   },
   "source": [
    "3. Name three advantages of the SELU activation function over ReLU.\n",
    "\n",
    "- SELU is self-normalizing under certain conditions (which avoids vanishing/exploding gradients):\n",
    "  - Input features must be standardized\n",
    "  - Hidden layer weights must be intialized with LeCun normal initialization\n",
    "  - Network architecture must be sequential (no skip connections or loops)\n",
    "  - Network connections must be dense\n",
    "- Takes on values < 0, which help make the average output closer to 0. This helps to reduce issue with vanishing gradients\n",
    "- Has nonzero gradient for values < 0\n",
    "- If a = 1, the function is smooth everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dt301kVUuh4"
   },
   "source": [
    "4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "- If runtime prediction latency is important\n",
    "  - leaky ReLU or ReLU is a good choice because libraries have optimizations for them since they are the most common choice. They are also very simple computations to start with\n",
    "- If runtime prediction latency matters less:\n",
    "  - If the network architecture allows for it, (basic dense feedforward MLP), then SELU is a good choice for its self-normalizing properties\n",
    "  - Otherwise, ELU has good training properties (no dead neurons like ReLU)\n",
    "- tanh can be useful when the output should be in the range (-1, 1) or in recurrent nets\n",
    "- logistic and softmax are useful as output layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIsqx8K-YMxH"
   },
   "source": [
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "\n",
    "The optimization algorithm might have too much \"memory\" of the predominant gradient from the past, and ignore current topography of the loss function, heading away from or past optima. Alternatively if the trajectory has gone through a pleteau, it might be hard to break out of the plateau by accelerating with current gradients because past memory been predominantly small gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGEWViX-ZAcj"
   },
   "source": [
    "6. Name three ways you can produce a sparse model.\n",
    "\n",
    "- Use l1 regularization\n",
    "- Use weight constraints that have boundary contours like a l1 regularization function\n",
    "- Use Tensorflow Model Optimization Toolkit (TF-MOT)\n",
    "\n",
    "The answer says that another option is to zero out small weights, but since the author mentions that this is likely to damage the model, it doesn't seem like a real viable option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kH2RX-skaXtm"
   },
   "source": [
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n",
    "\n",
    "If dropout slows down inference, it should be a constant overhead. It must generate a random mask for each neuron, and then mask the activation of each layer. An optimization would be to simply not perform the computation if the mask is set to 0 for a particular neuron, but it would be easiest to test whether this optimization is actually faster than masking empirically.\n",
    "\n",
    "In the same way backpropogation should be more efficient, because any neuron that is dropped out will not have any impact on the loss.\n",
    "\n",
    "On the other hand MC droput will be considerably slower, because it is slowed down by the number of sampled networks that are used."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNtIYBojI+JjZW33WIF5RTg",
   "collapsed_sections": [],
   "name": "hands_on_chapter_11_questions",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
