{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc0b0a63",
   "metadata": {},
   "source": [
    "1. How would you define clustering? Can you name a few clustering algorithms?\n",
    "\n",
    "Clustering is a class of unsupervised machine learning algorithms where the goal is to find subsets of data that are similar to each other.\n",
    "\n",
    "Some clustering algorithms: k-means, dbscan, gaussian mixture models, BIRCH, mean-shift, affinity propogation, spectral clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b7d607",
   "metadata": {},
   "source": [
    "2. What are some of the main applications of clustering algorithms?\n",
    "\n",
    "It is useful for:\n",
    "- segmentation \n",
    " - images\n",
    " - customers\n",
    "- data analysis\n",
    " - run data through a clustering algorithm and try to understand why the clusters were chosen\n",
    "- anomaly detection\n",
    " - data points that are not near cluster centers are likely to be outliers\n",
    "  - can use distance measures or density estimation\n",
    "- dimensionality reduction\n",
    "- preprocessing\n",
    " - can make data more linearly seperable\n",
    "- semi-supervised learning\n",
    " - propogate labels from representative data points to points within the same cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b68421a",
   "metadata": {},
   "source": [
    "3. Describe two techniques to select the right number of clusters when using K-Means .\n",
    "\n",
    "### find the inflection point in a graph of inertia\n",
    "\n",
    "inertia: mean squared distance of points to closest cluster centroid\n",
    "\n",
    "This is simple but innaccurate: just find the point where the \"elbow\" occurs in graph of k vs inertia\n",
    "\n",
    "\n",
    "### silhoutte digram\n",
    "Use diagram to choose k where clusters are of similar cardinality, have individual silhouette coefficients greater than global silhouette score\n",
    "\n",
    "silhouette coefficient:\n",
    "\n",
    "$$\n",
    "\\frac{b-a}{max(a, b)}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "a: mean distance to other instances in the same cluster\n",
    "\n",
    "b: mean distance to other instances of the closest cluster\n",
    "\n",
    "silhoutte score: mean of silhoutte coefficient accross all instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca989c31",
   "metadata": {},
   "source": [
    "4. What is label propagation? Why would you implement it, and how?\n",
    "\n",
    "Label propogation is useful when you have some labeled instances and some unlabelled instances. Since more data improves the performance and generalization of algorithms, and labelling data can be time consuming and costly, it is useful to find an automated process for labelling the data to improve supervised learning performance.\n",
    "\n",
    "If we assume that we start with a completely unlabelled dataset, label propogation can be performed by:\n",
    "- run the data through a clustering algorithm\n",
    "- find \"representative examples\" which are the points nearest the centroid of each cluster (or some number of those nearest the centroid per cluster)\n",
    "- label representative examples\n",
    "- propogate label of representative examples to other instances in their cluster\n",
    " - a distance threshold can improve the quality of labels. I imagine that probability density could also be a useful measure here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4394b20f",
   "metadata": {},
   "source": [
    "5. Can you name two clustering algorithms that can scale to large datasets? And two that look for regions of high density?\n",
    "\n",
    "Algorithms that can scale to large datasets:\n",
    "- k-means\n",
    "- agglomerative clustering\n",
    "- BIRCH if n_features < 20\n",
    "\n",
    "Algorithms that look for regions of high density:\n",
    "- DBSCAN\n",
    "- mean-shift\n",
    "- gaussian mixture models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da6b43",
   "metadata": {},
   "source": [
    "6. Can you think of a use case where active learning would be useful? How would you implement it?\n",
    "\n",
    "Active learning could be useful in pretty much any situation where data is much more plentiful than labels, but in particular in situations where labelling is very costly.\n",
    "\n",
    "To implement active learning\n",
    "- start with some small proportion of labels\n",
    "- train the model, which has some form of probability score or activation score per instance\n",
    "- predict the score per instance on unlabelled instances\n",
    "- ask a human for labels on the least certain instances\n",
    "- train the model again until returns diminish"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4becedd4",
   "metadata": {},
   "source": [
    "7. What is the difference between anomaly detection and novelty detection?\n",
    "\n",
    "Anomaly detection is trained on data that is assumed to contain outliers, and discovers instances that lie in areas of low probability density or areas that are distant from centroids or other points.\n",
    "\n",
    "Novelty detection assumes that the data does not contain anomalies during training, and then discovers novel points during the prediction phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29351cb",
   "metadata": {},
   "source": [
    "8. What is a Gaussian mixture? What tasks can you use it for?\n",
    "\n",
    "A gaussian mixture model is an unsupervised learning algorithm that assumes that data is generated from a mixture of gaussian distributions. There are k distributions each with mean and covariance, and each assigned a weight $\\phi_i$ that indicates the probability that the weight's corresponding distribution generated a point.\n",
    "\n",
    "In GMMs, only the data X is observed, while z is an unobserved latent variable that selects one of the k distributions. The process of training the distribution involves inferring the latent variable from the data via an iterative process called Expectation Maximization. In EM, two phases are repeated iteratively. First, assigning responsibilities (expectation over latent \"selector\" variable) of each cluster to each point, and second maximizing the likelihood of the evidence for each parameter (weights, means, and covariances).\n",
    "\n",
    "Bayesian GMMs assign priors to the parameters, but from a notational perspective we treat all of these random variables as part of the set of latent variables $Z$, and then use a method called Variational Inference to optimize the parameters. Variational Inference is very similar to EM, but can be used when the posterior distribution $p(z | X)$ is intractible. It makes the additional assumption that the posterior can be approximated by a variational distribution $q(Z)$ which factorizes according to a partitioning of the latent variables $Z$. Thus, $q(Z) = \\prod_{i=1}^{M} q_i(Z_i)$. Each factor $q_i$ can be optimized separately with respect to the other $q_j\\, \\text{where}\\, i \\neq j$. This is the mean field approach to Variational Inference.\n",
    "\n",
    "GMMs can be used for clustering, anomaly detection, and density estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945659a6",
   "metadata": {},
   "source": [
    "9. Can you name two techniques to find the right number of clusters when using a Gaussian mixture model?\n",
    "\n",
    "- Minimizing the model evidence k using the Bayesian Information Criterion or the Akaike Information Criterion\n",
    "- Using Bayesian GMM which discovers the optimal k as part of the optimization process, assuming the initial `n_components` is larger than the optimal k."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
