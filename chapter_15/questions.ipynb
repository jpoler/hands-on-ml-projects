{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad17238-9fec-4293-86ba-6adef2512799",
   "metadata": {},
   "source": [
    "1. Can you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN, and a vector-to-sequence RNN?\n",
    "\n",
    "sequence-to-sequence (assuming this could be direct sequence-to-sequence or encoder/decoder):\n",
    "- stock prices as mentioned in the book\n",
    "- weather or climate predictions\n",
    "- speech to text\n",
    "- machine translation\n",
    "- generative video/audio (from a prefix)\n",
    "- video frame by frame classification\n",
    "- mover trajectory prediction\n",
    "\n",
    "sequence-to-vector:\n",
    "- next-step predictions (many the same as above, just predicting the final frame)\n",
    "- sentiment score\n",
    "- any other scoring based on a sequence (reward/utility function in reinforcment learning?)\n",
    "- genome analysis\n",
    "- DALL E (description to image)\n",
    "\n",
    "vector-to-sequence:\n",
    "- image captioning\n",
    "- video/audio generation\n",
    "- robotic command processing (generate a list of actions from an enumerated command list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a264b3-41d7-44db-88f8-7e7f4ef73426",
   "metadata": {},
   "source": [
    "2. How many dimensions must the inputs of an RNN layer have? What does each dimension represent? What about its outputs?\n",
    "\n",
    "- An RNN must have 3D inputs: `[batch_size, steps, feature_dimensions]`\n",
    "- An RNN has 3D outputs: `[batch_size, steps, n_neurons]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c8ba8d-3095-4bb2-870b-8792e5c20ed4",
   "metadata": {},
   "source": [
    "3. If you want to build a deep sequence-to-sequence RNN, which RNN layers should have return_sequences=True ? What about a sequence-to-vector RNN?\n",
    "\n",
    "- sequence-to-sequence RNNs should have all layers set to return_sequences=True\n",
    "- sequence-to-vector RNNs should have all but the last RNN layer set to return_sequences=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e1ce9-162e-488e-81bc-5808519d2a83",
   "metadata": {},
   "source": [
    "4. Suppose you have a daily univariate time series, and you want to forecast the next seven days. Which RNN architecture should you use?\n",
    "\n",
    "A deep sequence-to-sequence RNN that uses a `Dense(7, ...)` layer at its top. Since the previous RNN layer will be configured with return_sequences=True, the Dense layer will receive a 3D tensor of shape `[batch_size, steps, n_neurons]`. `keras.layers.TimeDistributed` is not necessary, because if the rank of the input tensor to Dense is higher than 2, dense will perform the equivalent of 1D convolution with kernel size 1 across the time dimenension (index 1) (in other words it transforms the last axis dimension from n_rnn_neurons -> n_dense_neurons).\n",
    "\n",
    "The RNN layers could be LSTM or GRU. According to the author it is best to try both to see which performs best on a case-by-case basis.\n",
    "\n",
    "If the training data contains a very large number of time steps, training could take random, shorter windows from the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8e976-1de3-490d-932d-dde63784be7f",
   "metadata": {},
   "source": [
    "5. What are the main difficulties when training RNNs? How can you handle them?\n",
    "\n",
    "- vanishing/exploding gradients. With non-saturating activations, activations can grow or shrink at every timestep, eventually vanishing or exploding. Even with saturating activations like tanh gradients themselves can still vanish or explode.\n",
    "  - Gradient clipping\n",
    "  - Layer normalization, which normalizes activations according to the first and second moments across the feature dimension\n",
    "  - dropout\n",
    "- Short-term memory simple memory cells forget earlier states after very few timesteps (10 approximately)\n",
    "  - LSTM: Pass two hidden states forward in time: a long term memory and a short term memory. \n",
    "    - Learn gates that filter long term memory, inputs, and outputs based on input and previous hidden state.\n",
    "    - Also learn a gate for input activation\n",
    "  - GRU: Similar to LSTM but slightly simpler:\n",
    "    - Only one hidden state\n",
    "    - chooses between long-term memory and short-term inputs based on input and previous hidden state.\n",
    "  - 1D convolution in combination with RNN: downsample sequence to reduce sequence length while still providing features with similar information content\n",
    "  - WaveNet: Main idea is to increase dilation in successive layers so each top level neuron has a large, hierarchical receptive field even with only a branching factor (kernel size) of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235dfa05-6225-4886-87e8-3aad2ac33e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Can you sketch the LSTM cellâ€™s architecture?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f15337-c0f5-454c-970d-e9fb115a38fc",
   "metadata": {},
   "source": [
    "7. Why would you want to use 1D convolutional layers in an RNN?\n",
    "\n",
    "I actually answered this in question 5 already:\n",
    "\n",
    "> 1D convolution in combination with RNN: downsample sequence to reduce sequence length while still providing features with similar information content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de3565-24a5-4693-8d2a-3325375496d0",
   "metadata": {},
   "source": [
    "8. Which neural network architecture could you use to classify videos?\n",
    "\n",
    "I'm not positive what this question is asking. I'll assume the simplest interpretation which is it wants to group videos into categories like offensive/not-offensive. Another interpretation of the question would be frame-by-frame object detection/tracking, but I'll assume it isn't asking this.\n",
    "\n",
    "Assuming we want to predict whether content contains offensive material, the architecture would be sequence-to-vector.\n",
    "\n",
    "So we would likely have input data with shape `[batch_size, steps, width, height, channels]`. One possible approach would be to use:\n",
    "- A 2D convolutional network to reduce the spatial dimensionality while increasing feature depth\n",
    "- GlobalAveragePooling to eliminate the spatial dimension of each feature map\n",
    "- An RNN that takes the output of the convolutional network `[batch_size, steps, n_feature_maps]` with a `Dense(1, activation=\"softmax\")` top layer\n",
    "\n",
    "I looked in to this more after sketching out the above architecture, and it looks like the above is the most naive architecture described in [Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset](https://arxiv.org/pdf/1705.07750.pdf)\n",
    "\n",
    "More sophisticated approaches use 3D convolution and 2 streams that combine information from RGB frames and precomputed optical flow frames."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
