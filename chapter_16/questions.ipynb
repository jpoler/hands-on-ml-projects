{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee7b0d7-617b-4b95-8c78-710e0b19e6e7",
   "metadata": {},
   "source": [
    "1. What are the pros and cons of using a stateful RNN versus a stateless RNN?\n",
    "\n",
    "A stateful RNN can learn from much longer sequences by passing its state to the next training step, but does not face the issue of having to backpropogate gradients over long sequences.\n",
    "\n",
    "The main drawback is that structuring training can be trickier, since it is harder, but possible, to perform training across batched sequences. Since it is stateful, the RNN must train on consecutive subsequences, meaning that shuffling is constrained. Batches can contain subsequences of the ith sequence at the ith offset from within a batch, and then shuffling can take place across these batched sequences (inter sequence but not intra sequence). Since shuffling is constrained, the dataset will somewhat violate the IID assumption of gradient descent (although that constraint is already somewhat violated by taking multiple passes through a dataset).\n",
    "\n",
    "If sequences are already short, there may not be a need for a stateful RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0fc6ad-e10c-4200-a714-93442fea9c7f",
   "metadata": {},
   "source": [
    "2. Why do people use Encoder–Decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
    "\n",
    "One clear advantage of an Encoder-Decoder architecture is that it decouples the lengths of the input and output sequence, so they can be of a different size.\n",
    "\n",
    "It also seems to add a bit of control. Variational Autoencoders for example can treat the latent state like a random variable and do other transformations. Latent state can also be used for interpretability by studying the structure of the latent state space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87db6911-c7d9-492d-9565-776764aa2527",
   "metadata": {},
   "source": [
    "3. How can you deal with variable-length input sequences? What about variable-length output sequences?\n",
    "\n",
    "If instances within a batch must all be of the same lenght but they vary, padding can be added. Then, masking can be used to ignore any padded item in a sequence. During forward propagation, a layer will simply pass its input to its output if the value from the previous time step. During calculation of the loss function, padded sequence elements will be ignored, and so padding will not affect backpropagation or the application of gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51357296-4d45-4517-8452-444d138d0385",
   "metadata": {},
   "source": [
    "4. What is beam search and why would you use it? What tool can you use to implement it?\n",
    "\n",
    "Since Encoder-Decoder architectures can track the conditional probability of the output sequence given the input sequence, the model is able to score the likelihood of a partial output sequence given it's partial input sequence.\n",
    "\n",
    "Beam search keeps track of the K most probably sequences while generating output sequences. At each step, the model extends each of the K sequences with all possible extensions, and then evalutates the probability of all extended sequences, keeping the top K, regardless of which sequence they came from (for example one sequence could produce all of the the top K sequences at a particular time step).\n",
    "\n",
    "Beam search is useful because often the most probable sequence at the end was not the most probably length 1 sequence (more generally length i), and so by tracking K sequences, the chances of discovering the more probable sequences are increased.\n",
    "\n",
    "Beam search can be implemented using `tfa.seq2seq.beam_search_decoder.BeamSearchDecoder` from TensorFlow Addons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7abaa9-aede-4c98-80e8-4653915486c3",
   "metadata": {},
   "source": [
    "5. What is an attention mechanism? How does it help?\n",
    "\n",
    "An attention mechanism allows a model to focus on any part of the input sequence (often the input sequence is really the output sequence of an encoder) when generating an element in the output sequence. This sidesteps the issue that RNNs face with propagating memories across long sequences.\n",
    "\n",
    "Implementation of attention mechanisms can vary in both architecture and in how they are calculated. The author gives three examples, one an Encoder-Decoder RNN with an alignment model using concatenative attention, the same model but with simplifications and multiplicative attention (dot-products), and the other being the transformer.\n",
    "\n",
    "concatenative attention: concatenate encoders output at time t with decoder hidden state from time t-1 and pass values for all t through a time distributed dense layer and then softmax, using the softmax outputs as weights to scale encoder outputs passed to a decoder memory cell at time t\n",
    "\n",
    "multiplicative attention: compute dot product instead of concatenation in the above scheme, and pass those values through the softmax layer. Some variants are possible like using the current hidden state at time t instead of t-1, and then use the attention mechanism to directly compute decoder output.\n",
    "\n",
    "transformer: Will discuss in the next question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f572a47-af47-4850-966a-afe615554a68",
   "metadata": {},
   "source": [
    "6. What is the most important layer in the Transformer architecture? What is its purpose?\n",
    "\n",
    "The multi-headed attention layer. First, to define scaled dot-product attention:\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = softmax(\\frac{QK^\\intercal}{\\sqrt{d_{keys}}})V\n",
    "$$\n",
    "\n",
    "- $Q$ is a matrix of shape $[n_{queries}, d_{keys}]$ and where n_queries is the number of queries and $d_{keys}$ is the dimensionality of each query and key\n",
    "- $K$ has shape $[n_{keys}, d_{keys}]$ and contains the number of keys and values (length of sequence input into scaled dot product attention layer)\n",
    "- $V$ has shape $[n_{keys}, d_{values}]$ where $d_{values}$ is the dimensionality of each value\n",
    "- Similar to temperature, dividing by the square root of the dimensionality of the keys prevents the softmax from saturating. This keeps gradients from dissappearing\n",
    "- In the encoder $Q$, $K$, and $V$ are all equal to the list of input words (compare similarity of input embeddings)\n",
    "- In the masked multi-headed attention layer in the decoder $Q$, $K$, and $V$ are all equal to the list of target words, but with causal masking\n",
    "- In the masked multi-headed attention layer in the decoder $K$, and $V$ the word encodings produced by the encoder, where $Q$ is the list of word encodings produced by the decoder.\n",
    "\n",
    "A multi-headed attention layer:\n",
    "- Passes $Q$ $K$ and $V$ into a stack of \n",
    "- $h$ time-distributed Dense layers without bias\n",
    "- $h$ scaled dot product attention layers\n",
    "- concatenates the $h$ outputs\n",
    "- computes another time-distributed Dense layer without bias\n",
    "\n",
    "Each of the $h$ layers computes a different feature of the input sequence, analagous to feature maps in conv nets.\n",
    "\n",
    "I'm citing the book because I borrowed so heavily here:\n",
    "\n",
    "Géron, Aurélien. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (p. 559). O'Reilly Media. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5d2f85-a5f1-4a55-8e43-e25eac33c895",
   "metadata": {},
   "source": [
    "7. When would you need to use sampled softmax?\n",
    "\n",
    "If the output vocabulary is very large, training can be prohibitively expensive due to the need to compute softmax over target word in the vocabulary. To avoid this it is possible to compute softmax on the known true target, along with a sampled subset of incorrect words. This speeds up training.\n",
    "\n",
    "At inference time the actual softmax must be computed over the full vocabulary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
