{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c88cbe65-fd2b-4892-88d1-cf737377cf0c",
   "metadata": {},
   "source": [
    "3. Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of threshold logic units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?\n",
    "\n",
    "One reason is that logistic regression uses a sigmoid output in the range [0, 1], so it can be interpreted as a probability.\n",
    "\n",
    "A perceptron is very similar to a logistic classifier. The former can be converted to the latter by switching the output unit to a sigmoid and using an appropriate optimization technique like gradient descent on a cross-entropy loss function or Iterated Reweighted Least Squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5eba3-4457-490a-b40e-db42b842be43",
   "metadata": {},
   "source": [
    "4. Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "\n",
    "The biggest reason is because it is differentiable because it doesn't have discontinuities like the heaviside step function. It also has a gradient everywhere and so is not as susceptible to plateaus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d767e9ab-5ee2-4da5-b750-f090d2904e9b",
   "metadata": {},
   "source": [
    "5. Name three popular activation functions. Can you draw them?\n",
    "\n",
    "ReLu, softmax, and sigmoid. There's also tanh, leaky relu, softplus, softsign, selu, and elu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4ba4a5-8b36-411f-9c37-3608403335c1",
   "metadata": {},
   "source": [
    "6. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "\n",
    "What is the shape of the input matrix X ?\n",
    "\n",
    "$X \\in \\mathbb{R}^{m\\times10}$\n",
    "\n",
    "What are the shapes of the hidden layer’s weight vector W h and its bias vector b h ?\n",
    "\n",
    "$W_h \\in \\mathbb{R}^{10\\times50}$\n",
    "\n",
    "$b_h \\in \\mathbb{R}^{50}$\n",
    "\n",
    "What are the shapes of the output layer’s weight vector W o and its bias vector b o ?\n",
    "\n",
    "$W_o \\in \\mathbb{R}^{50\\times3}$\n",
    "\n",
    "$b_o \\in \\mathbb{R}^{3}$\n",
    "\n",
    "What is the shape of the network’s output matrix Y ?\n",
    "\n",
    "$X \\in \\mathbb{R}^{m\\times3}$\n",
    "\n",
    "\n",
    "Write the equation that computes the network’s output matrix Y as a function of X , W h , b h , W o , and b o .\n",
    "\n",
    "$$\n",
    "Y = Relu( Relu(XW_h + b_h)W_o + b_o )\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f55c7-d5d4-40b5-ba5f-bda5b3c314c0",
   "metadata": {},
   "source": [
    "7. How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, and which activation function should you use? What about for getting your network to predict housing prices, as in Chapter 2 ?\n",
    "\n",
    "1 neuron with a sigmoid activation for binary classification.\n",
    "\n",
    "10 neurons with softmax activation for multiclass classification.\n",
    "\n",
    "1 neuron with no activation function (identity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7496d248-b3a4-4eba-8750-16eed5317ac0",
   "metadata": {},
   "source": [
    "8. What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?\n",
    "\n",
    "Backpropagation allows gradients to be computed in an efficient fashion via the chain rule. It is a form of dynamic programming.\n",
    "\n",
    "It works by first forward propagating neuron activations through the network, and caching the resulting computations. These cached values are used during the backpropagation phase, where gradients are propagated backwards, starting with the change in the cost function with respect to the network outputs, and then computing the change in the cost function with respect to activations and weights at each neuron, using the chain rule in combination with cached gradients from all of a neuron's connected outputs to effeciently propagate gradients back through the network. The resulting gradients with respect to weights and biases are then used for gradient descent by updating the weights and biases in the directon of the negative gradient.\n",
    "\n",
    "A mathmatical description is a bit more satisfying but also tedious to type out and is covered in countless textbooks and blog posts, for example Bishop (2006) chapter 5.3.\n",
    "\n",
    "Reverse mode autodifferentiation is able to do the same thing as backpropagation without the need for the programmer to explicitly compute gradients. It works by representing forward propagation as compuational graph. Each operation must also specify what its gradient is. Then a computational graph representing backpropagation can be constructed. This is a symbolic representation of backpropagation, that when evaluated results in numeric computation of the gradient.\n",
    "\n",
    "After comparing my answer to the author's I'll point out that the introduction to Bishop (2006) chapter 5 notes that backpropagation is an overloaded term. I answer this question using Bishop's definition, where backpropagation is meant to convey the algorithm for passing gradients back through a network. Auerelion uses backpropagation to reflect the overall gradient descent algorithm. I think I prefer Bishop's definition because one could always say \"gradient descent with backpropagation\", but the other way around seems less meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987601a7-bedf-41a4-bc3f-42a947407c77",
   "metadata": {},
   "source": [
    "9. Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "\n",
    "Just going off of my memory from the playground:\n",
    "- # of layers\n",
    "- # neurons per layer\n",
    "- learning rate\n",
    "- activation function per layer\n",
    "- regularization function (l1, l2, ...) (could be per layer)\n",
    "- regularization coefficient\n",
    "- batch size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
