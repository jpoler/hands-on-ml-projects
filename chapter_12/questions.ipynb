{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chapter_12_questions.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNNRIQivytN29Xxs11bbdRy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1. How would you describe TensorFlow in a short sentence? What are its main features? Can you name other popular Deep Learning libraries?\n","\n","Tensorflow is a numerical computing library that offers an API similar to numpy.\n","\n","features include:\n","- multi-dimensional array operations via an API similar to numpy\n","- can use CPU, GPU, or TPU backends\n","- distributed training\n","- automatic computational graph generation\n","- reverse-mode autodifferentiation\n","- computational graphs are portable across languages and OSes\n","\n","Other deep learning libraries are Keras (Tensorflow also supports Keras), PyTorch, TensorRT, MXNet, Theano, Deeplearning4j, Caffe, Chainer, and JAX."],"metadata":{"id":"6x1v4TdyZ8_e"}},{"cell_type":"markdown","source":["2. Is TensorFlow a drop-in replacement for NumPy? What are the main differences between the two?\n","\n","While the APIS are similar, they aren't swappable. As mentioned previously, TensorFlow was written with GPUs and TPUs in mind, and more easily scales to distributed training. TensorFlow compiles down to computational graphs, which can be optimized and run in parallel. Python TensorFlow can analyze Python functions and translate them to TensorFlow graphs. One of the key features offered by computational graphs is that the can be used for autodifferentiation for gradient computation. These features go beyond the core numpy API."],"metadata":{"id":"CLa4dvxQcu79"}},{"cell_type":"markdown","source":["3. Do you get the same result with tf.range(10) and tf.constant(np.arange(10)) ?\n","\n","No, `tf.range(10)` has `dtype=int32` where `tf.constant(np.arange(10))` has `dtype=int64`"],"metadata":{"id":"oBSebBUSf-d-"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","\n","print(tf.range(10))\n","print(tf.constant(np.arange(10)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lXL3N92_go6y","executionInfo":{"status":"ok","timestamp":1656809039732,"user_tz":240,"elapsed":4,"user":{"displayName":"Jon Poler","userId":"13423372355356044593"}},"outputId":"2e368340-9f3d-4f36-9342-aab05e358467"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)\n","tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int64)\n"]}]},{"cell_type":"markdown","source":["4. Can you name six other data structures available in TensorFlow, beyond regular tensors?\n","\n","- string tensor: represent byte strings\n","- tensor arrays: lists of tensors of same shape and data type\n","- ragged tensors: a tensor with one or more \"ragged dimensions\" (nonuniform)\n","- queues: tensors representing queue data\n","- sparse tensors: tensors containing mostly zeros\n","- sets: tensors representing set data structures\n"],"metadata":{"id":"SOddN22_hKpV"}},{"cell_type":"markdown","source":["5. A custom loss function can be defined by writing a function or by subclassing the keras.losses.Loss class. When would you use each option?\n","\n","To save and load a loss that takes parameters, it would be necessary to subclass keras.losses.Loss"],"metadata":{"id":"qemD-Pu9jK-E"}},{"cell_type":"markdown","source":["6. Similarly, a custom metric can be defined in a function or a subclass of keras.metrics.Metric . When would you use each option?\n","\n","The same use-case would require a custom metric: when hyperparameters need to be persisted across save and load boundaries.\n","\n","Additionally, there may be cases where existing base metrics like Mean might not meet the use-case because the metric isn't an average of batch metrics. In this cass subclassing Metric is the correct thing to do."],"metadata":{"id":"F0WFbR77kGFD"}},{"cell_type":"markdown","source":["7. When should you create a custom layer versus a custom model?\n","\n","Although Model is a subclass of Layer, it is generally best to subclass Layer when creating a new layer type or a block of Layers, and subclass Model when creating an entire custom model. This keeps the abstraction clean and keeps the ideas conceptually separate."],"metadata":{"id":"mTccHtH7oaLL"}},{"cell_type":"markdown","source":["8. What are some use cases that require writing your own custom training loop?\n","\n","- more than one optimizer\n","- if unsure of details of `fit` now or in the future\n","- reinforcement learning might require more control (I'll find out soon enough)"],"metadata":{"id":"i8i7d1QfpWrh"}},{"cell_type":"markdown","source":["9. Can custom Keras components contain arbitrary Python code, or must they be convertible to TF Functions?\n","\n","They can contain arbitrary python code, but at a cost:\n","- code with side effects will only be executed during the trace phase of autograph, and will not execute during graph execution afterwards (one trace per data type / shape)\n","- the alternative is to force the component to be \"dynamic\", which will be slower for computationally intensive tasks"],"metadata":{"id":"j05KfUi0q0hS"}},{"cell_type":"markdown","source":["10. What are the main rules to respect if you want a function to be convertible to a TF Function?\n","\n","- try to use tf.* functions wherever possible\n","- do not use code with side effects\n","- in particular non tf.* random functions will return the same result every time\n","- must create tf.Variables on first call (better to pass them in at top-level)\n","- source code should be available, else graph generation will fail\n","- use tf.range, not range unless it is a static loop on purpose\n","- prefer vectorized implementation over loops"],"metadata":{"id":"yPL1Sg6arYGR"}},{"cell_type":"markdown","source":["11. When would you need to create a dynamic Keras model? How do you do that? Why not make all your models dynamic?\n","\n"," This wasn't obvious to me, but the author suggests that this would be useful to debug models, in particular by gaining access to a debugger. I lean towards print statements, but dynamic mode would also allow for this possibility."],"metadata":{"id":"iRlXatuSsR5S"}}]}