{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c707f975",
   "metadata": {},
   "source": [
    "1. What is the fundamental idea behind Support Vector Machines?\n",
    "\n",
    "The idea is to maximize the margin between separate classes, where the margin is defined as the distance between the closest two points from differing classes. SVM is usually only defined for binary classification, and takes the form:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\begin{cases} \n",
    "    0 & \\mathbf{w}^\\intercal \\mathbf{x} + b < 0 \\\\\n",
    "    1 & \\mathbf{w}^\\intercal \\mathbf{x} + b >= 0  \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "In the idealized case, data is linearly seperable, and so the optimization becomes minimizing the length of $\\lVert w \\rVert$ as part of a Lagrangian optimization problem:\n",
    "\n",
    "$$\n",
    "\\underset{w, b}{\\operatorname{\\min}} \\frac{1}{2}\\lVert w \\rVert\n",
    "$$\n",
    "\n",
    "with constraints\n",
    "\n",
    "$$\n",
    "t^{(i)}(\\mathbf{w}^\\intercal \\mathbf{x} + b) >= 1 \\quad\\text{for}\\quad  i = 1, 2, ... , m\n",
    "$$\n",
    "\n",
    "where $t^{(i)}$ is the target class.\n",
    "\n",
    "In practice, data will not be linear seperable, so we introduce slack variables $\\zeta^{(i)}$, for each data point, and minimize with $\\zeta^{(i)}$ jointly with $w$ and $b$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\underset{w, b, \\zeta}{\\operatorname{\\min}} \\frac{1}{2}\\lVert w \\rVert + C\\sum\\limits_{i=1}^{n}\\zeta^{(i)}\n",
    "$$\n",
    "\n",
    "with constraints\n",
    "\n",
    "$$\n",
    "t^{(i)}(\\mathbf{w}^\\intercal \\mathbf{x} + b) >= 1 - \\zeta^{(i)} \\quad\\text{and}\\quad \\sum\\limits_{i=0}^{m}\\zeta^{(i)} >= 0 \\quad\\text{for}\\quad  i = 1, 2, ... , m\n",
    "$$\n",
    "\n",
    "This encourages the optimization to keep keep $\\lVert w \\rVert$ small, which decreases the slope of the decision surface, and also to keep the number of datapoints on the wrong side of the margin (based on their class) to a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82888a9a",
   "metadata": {},
   "source": [
    "2. What is a support vector?\n",
    "\n",
    "A support vector is a vector that lies within the margin. Only the support vectors influence the location of the margin, where non-support vectors do not influence the boundary.\n",
    "\n",
    "Significantly, if we view SVM as its dual, we have:\n",
    "\n",
    "$$\n",
    "\\hat{w} = \\sum\\limits_{i=1}^{m}\\hat{a}^{(i)}t^{(i)}\\mathbf{x}^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{b} = \\frac{1}{n_s}\\sum\\limits_{i=1}^{m}(t^{(i)} - \\mathbf{w}^\\intercal \\mathbf{x})\n",
    "$$\n",
    "\n",
    "And we can see that $a^{(i)} \\neq 0$ when $x^{(i)}$ is a support vector, and so we only need to evaluate the support vector terms when making predictions, which will be relatively few of the points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8428f652",
   "metadata": {},
   "source": [
    "3. Why is it important to scale the inputs when using SVMs?\n",
    "\n",
    "If scales vary significantly between dimensions, SVM is very constrained in the low variance dimensions if the distance between support vectors is small in those dimensions, and so may not be able to choose a margin that accounts for the high variance dimensions well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebcb1aa",
   "metadata": {},
   "source": [
    "4. Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?\n",
    "\n",
    "It can output a confidence score, which is simply:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^\\intercal \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "SVM is not a probablistic classifer.\n",
    "\n",
    "The book answer includes that you can use the parameter `probability=True` in order to make SVM have a probabilistic interpretation. This is done via applying logisitic regression to the output confidence scores and calibrating those scores with an additional round of 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff42e1c",
   "metadata": {},
   "source": [
    "5. Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?\n",
    "\n",
    "Since the number of instances is two orders of magnitude greater than the number of features, the primal form of SVM should be used. Linear SVC is $O(mn)$ where the dual form has time complexity between $O(m^2n)$ and $O(m^3n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf0690",
   "metadata": {},
   "source": [
    "6. Say you’ve trained an SVM classifier with an RBF kernel, but it seems to underfit the training set. Should you increase or decrease γ ( gamma )? What about C ?\n",
    "\n",
    "Gamma and C have the same behavior with respect to their magnitude. Increasing them reduces the regularization, decreasing them increase the regularization. So in this case since we are underfitting, we could consider increase gamma, or C, although increasing them both at the same time would be less informative than increase one in isolation, or better grid or random searching higher values for both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74cf9e",
   "metadata": {},
   "source": [
    "7. How should you set the QP parameters ( H , f , A , and b ) to solve the soft margin linear SVM classifier problem using an off-the-shelf QP solver?\n",
    "\n",
    "\n",
    "$$\n",
    "H = I_{n_p} \\quad\\text{except}\\quad I_1 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_i = \\frac{C\\zeta^{(i)}}{w_i} \\quad\\text{where}\\quad f_1 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{(i)} = -t^{(i)}\\mathbf{\\dot{x}}^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{b}_{i} = \\zeta^{(i)} - 1\n",
    "$$\n",
    "\n",
    "The above was my original answer. The algebra seems to work but the problem is that it does not enforce the set of constraints $\\zeta^{(i)} \\geq 0$\n",
    "\n",
    "I won't copy the book answer but in retrospect it does make sense that all parameters should be part of $\\mathbf{p}$, so making $\\mathbf{p}$ have $n + m +  1$ parameters, which now includes $\\zeta^{(i)}$, and $n_c$ having $2m$ parameters does make more sense."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
