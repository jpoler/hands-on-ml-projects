{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82a66408",
   "metadata": {},
   "source": [
    "1. Which Linear Regression training algorithm can you use if you have a training set with millions of features?\n",
    "\n",
    "Stochastic Gradient Descent. The normal equations are O(n^2.4) at best where n is the number of features, so for millions of parameters this is likely intractable. SGD is O(n) per step, and will be a convex optimization problem so the runtime will be considerably less."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b3e292",
   "metadata": {},
   "source": [
    "2. Suppose the features in your training set have very different scales. Which algorithms might suffer from this, and how? What can you do about it?\n",
    "\n",
    "Most algorithms will suffer from mismatch scales. Any algorithm optimized by SGD will. SVM is sensitive to scales as well. Decision trees are one example of scale-insensitive algorithms.\n",
    "\n",
    "To deal with this, one can use StandardScaler to normalize the data to 0 mean and unit variance. Alternatively MinMaxScaler will scale the data to range from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f7857d",
   "metadata": {},
   "source": [
    "3. Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?\n",
    "\n",
    "No. Logistic Regression has a convex cost function, and so there is only a global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9f425b",
   "metadata": {},
   "source": [
    "4. Do all Gradient Descent algorithms lead to the same model, provided you let them run long enough?\n",
    "\n",
    "No. SGD or mini-batch SGD will behave differently depending on the order they process the data. If the cost function has local minima or other critical points like saddle points, the optimization can lead to different minima.\n",
    "\n",
    "Further, if early stopping is used, the algorithm might halt in different places depending on the stochastic path followed after the max iterations when not yet converged to a local/global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4775931",
   "metadata": {},
   "source": [
    "5. Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?\n",
    "\n",
    "If the training error is not going up, then it is likely that the problem is overfitting.\n",
    "\n",
    "If the training error is also going up, then the learning rate might be too high. One way to find a reasonable learning rate is to try exponentially increasing learning rates on a small portion of the dataset starting from 1e-5 and see where the cost function explodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77d1cfe",
   "metadata": {},
   "source": [
    "6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?\n",
    "\n",
    "\n",
    "No. It is better to set a threshold for number of iterations that the validation error has been consistenly increasing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c016af",
   "metadata": {},
   "source": [
    "7. Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?\n",
    "\n",
    "\n",
    "SGD will usually reach the vicinity of a minimum the fastest in terms of the total amount of data utilized. Batch gradient descent will actually converge. To make SGD actually converge, decrease the learning rate over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b86a0",
   "metadata": {},
   "source": [
    "8. Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?\n",
    "\n",
    "Overfitting is likely the cause. Three potential solutions are:\n",
    "- Introduce a regularizer. Some choices are l2 (Ridge), l1 (LASSO), or l1 + l2 (ElasticNet)\n",
    "- Reduce the capacity of the model by reducing the degree of the polynomial transformer\n",
    "- If using gradient descent, try early stopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae66a540",
   "metadata": {},
   "source": [
    "9. Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter Î± or reduce it?\n",
    "\n",
    "High bias. Try reducing alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3d08db",
   "metadata": {},
   "source": [
    "10. Why would you want to use:\n",
    "- Ridge Regression instead of plain Linear Regression (i.e., without any regularization)?\n",
    "\n",
    "To reduce the capacity of the model to cope with overfitting (high variance).\n",
    "\n",
    "- Lasso instead of Ridge Regression?\n",
    "\n",
    "To encourage the model to favor sparse parameter solutions.\n",
    "\n",
    "- Elastic Net instead of LASSO?\n",
    "\n",
    "To still favor a somewhat spare solution, but also to have the favorable convergence properties of Ridge regression. ElasticNet is favorable over lasso because LASSO can behave erratically when the number of features is greater than the number of training instances or when some features are strongly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f414a85",
   "metadata": {},
   "source": [
    "11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two Logistic Regression classifiers or one Softmax Regression classifier?\n",
    "\n",
    "Two Logistic Regression classifiers. The setting and time-of-day classes are not mutually exclusive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
